# Project: Data Pipelines with Airflow

## Introduction

A fictional music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines by using [Apache Airflow](https://airflow.apache.org/).

The source data resides in Amazon S3 buckets and needs to be processed in Sparkify's data warehouse in Amazon Redshift.

After setting up an AWS Redshift DB, the task is building an ETL pipeline that:

- extracts the JSON data from S3
- processed them, and
- loads them back to S3 as a set of dimensional analytics tables.

The data pipeline has to be dynamic, built from reusable tasks, can be monitored, allows easy backfills, and conducts automated data quality checks. The pipline steps / tasks are as follows:

![pipeline graph view](DAG_graph_view.JPG)

## Input data

Data is collected for song metadata and user activities, using JSON files. The data  two datasets that reside in AWS S3. Here are the S3 links for each:

- Song data: `s3://udacity-dend/song_data`
- Log data: `s3://udacity-dend/log_data`

Log data json path: `s3://udacity-dend/log_json_path.json`

### Song dataset format

The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset:

- `song_data/A/B/C/TRABCEI128F424C983.json`
- `song_data/A/A/B/TRAABJL12903CDCF1A.json`

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```json
{
  "num_songs": 1,
  "artist_id": "ARGSJW91187B9B1D6B",
  "artist_latitude": 35.21962,
  "artist_longitude": -80.01955,
  "artist_location": "North Carolina",
  "artist_name": "JennyAnyKind",
  "song_id": "SOQHXMF12AB0182363",
  "title": "Young Boy Blues",
  "duration": 218.77506,
  "year": 0
}
```

### Log dataset format

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

`log_data/2018/11/2018-11-12-events.json`
`log_data/2018/11/2018-11-13-events.json`

A single entry has the following structure:

```json
{
  "artist": "Survivor",
  "auth": "Logged In",
  "firstName": "Jayden",
  "gender": "M",
  "itemInSession": 0,
  "lastName": "Fox",
  "length": 245.36771,
  "level": "free",
  "location": "New Orleans-Metairie, LA",
  "method": "PUT",
  "page": "NextSong",
  "registration": 1541033612796,
  "sessionId": 100,
  "song": "Eye Of The Tiger",
  "status": 200,
  "ts": 1541110994796,
  "userAgent": "\"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"",
  "userId": "101"
}
```

## Schema

Final fact and dimension tables should be following a star schema with an analytics focus. It is defined as follows:

### Fact Table

**songplays** - records in log data associated with song plays i.e. records with page NextSong
*songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent*

### Dimension Tables

**users** - users in the app
*user_id, first_name, last_name, gender, level*

**songs** - songs in music database
*song_id, title, artist_id, year, duration*

**artists** - artists in music database
*artist_id, name, location, lattitude, longitude*

**time** - timestamps of records in songplays broken down into specific units
*start_time, hour, day, week, month, year, weekday*

## Build

Python 3.6. or higher - Used to code DAG's and its dependecies
Apache Airflow 1.10.2 - Workflows platform

## Config

AWS credentials and Redshift connection are managed directly in Airflow's UI.

## Run

Prerequisites: Access to AWS and an Amazon Redshift cluster.

1. Set up a Redshift database and create the necessary tables.
2. Put project files in their respective folders in an Airflow installation.
3. Adjust parameters in the DAG script, spakify_dag.py, as desired.
4. Set aws_credentials and redshift connection in Airflow.
5. Launch sparkify_dag from the Airflow UI.

## Scripts in Repo

- `create_tables.sql` - Contains DDL for all tables (provided)
- `sparkify_dag.py` - The DAG configuration file to run in Airflow
- `stage_redshift.py` - Custom operator to read files from S3 and load into Redshift staging tables
- `load_fact.py` - Custom operator to load the fact table in Redshift
- `load_dimension.py` - Custom operator to read from staging tables and load the dimension tables in Redshift
- `data_quality.py` - Custom operator for data quality checking
